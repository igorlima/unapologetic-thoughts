{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4581b95f-ee6f-46ee-ba6d-75e61f6db310",
   "metadata": {},
   "source": [
    "# AI Prompts\n",
    "\n",
    "- re-render Markdown preview after an edition:\n",
    "  - __\"Activate Command Pallete\"__ by pressing `Ctrl+Shift+C`\n",
    "    - _it's also located under the menu \"View\"_\n",
    "  - type __\"Render All Markdown Cells\"__\n",
    "  - press `Enter`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc8987-e773-42a2-8554-4ff6bec0e258",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c15ee4-5757-4fe5-a84e-aaff8247795b",
   "metadata": {},
   "source": [
    "\n",
    "- How to Create a Virtual Environment and Use it on Jupyter Notebook\n",
    "  - https://towardsdatascience.com/how-to-create-a-virtual-environment-and-use-it-on-jupyter-notebook-6c0b7b1cfca0\n",
    "  - https://medium.com/@kishanck/virtual-environments-for-jupyter-notebooks-847b7a3b4da0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae0fbe7-7848-40fb-a950-4ee1f2c715ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using passwords in Jupyter notebooks\n",
    "\n",
    "In Python you can ask the user for input via the `input` function.\n",
    "```python\n",
    "pwd = input(\"Password:\")\n",
    "```\n",
    "\n",
    "When you run this command locally, here's what it might look like:\n",
    "\n",
    "```\n",
    ">>> pwd = input(\"Password:\")\n",
    "Password: supersecret\n",
    "```\n",
    "\n",
    "The `pwd` variable will contain the string \"supersecret\", but notice how the command prompt actually shows what the user is typing! That means that somebody who is sitting next to you, or looking at your screen over zoom, also can read your password! That's bad.\n",
    "\n",
    "#### getpass\n",
    "\n",
    "For situations like this one, you may enjoy using the getpass module in Python instead. It has the same functionality but won't display the typed password.\n",
    "\n",
    "```\n",
    ">>> import getpass\n",
    ">>> pwd = getpass.getpass(\"give password\")\n",
    "Password:🗝️\n",
    "```\n",
    "\n",
    "No matter what you type, it won't be printed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf2730-0609-46df-bd59-375c58317329",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3df19-c1b6-48d8-be0f-c10cc5e9dc5e",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eccae4-09ea-42db-b8a1-ed3d7ac2253e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f97c55-251f-4893-8342-062bf7bf9e65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cabad1f-e4f7-45e1-8d93-a6c3ef886d3e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter GOOGLE_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ['GOOGLE_API_KEY'] = getpass.getpass(\"enter GOOGLE_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3980015e-b849-4bd0-8115-3bab1af7df53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pip3 install -I google-generativeai==0.5.2\n",
    "# pip3 install --upgrade --force-reinstall google-generativeai\n",
    "# pip3 show google-generativeai\n",
    "# pip3 index versions google-generativeai\n",
    "\n",
    "# Overview of Generative AI on Vertex AI:\n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/learn/overview\n",
    "# Google Gemini Documentation:\n",
    "# https://github.com/google/generative-ai-docs?tab=readme-ov-file\n",
    "\n",
    "import google.generativeai as genai\n",
    "import google.ai.generativelanguage as glm\n",
    "import os\n",
    "\n",
    "GOOGLE_API_KEY=os.environ.get(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c55a5f01-594a-4088-bf04-1e4af9113987",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# https://github.com/sigoden/aichat/blob/1e8fc5d269985048d8d3023a615b94a8908571cf/models.yaml#L79\n",
    "for model in genai.list_models():\n",
    "  # pprint.pprint(model)\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c27937-9d61-4df5-b765-e574995d52f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c1b89-7784-4a1d-9315-8ca1e72ee5e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile prompt.txt\n",
    "What's your name? Is it Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae0207-f63b-4858-9745-e9cb2e17945f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "PROMPT = \"\"\n",
    "with open(\"prompt.txt\", \"r\") as file:\n",
    "  content = file.read()\n",
    "  PROMPT=content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a98f817d-0061-43f3-90d6-dda4c78bdd4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, my name is Gemini, a multi-modal AI language model developed by Google. My name is not Gemini.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\n",
    "What's your name? Is it Gemini\n",
    "\\\"\"\"\n",
    "\"\"\"\n",
    "CONTEXT = None\n",
    "\n",
    "prompt = PROMPT\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "# response = model.generate_content(prompt)\n",
    "response = model.generate_content(glm.Content(\n",
    "  parts = [\n",
    "    glm.Part(text=prompt),\n",
    "    glm.Part(text=CONTEXT if CONTEXT is not None else ''),\n",
    "  ],\n",
    "))\n",
    "print(response.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f029c-a3ec-4ba5-a3da-1065aba89039",
   "metadata": {},
   "source": [
    "## Cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422abc53-123d-4577-93f5-9f7db26098ca",
   "metadata": {},
   "source": [
    "Calls made using Trial keys are free of charge. Trial keys are rate-limited, and cannot be used for commercial purposes.\n",
    "- https://dashboard.cohere.com/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd23728-2be9-4ced-b30c-2771b762a22a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7686abe2-7430-461d-9f25-6069b4d51fee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter COHERE_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ['COHERE_API_KEY'] = getpass.getpass(\"enter COHERE_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9d98f95-f330-4a6d-936f-8c41554f10ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pip3 install -I cohere==5.5.0\n",
    "# pip3 install --upgrade --force-reinstall cohere\n",
    "# pip3 show cohere\n",
    "# pip3 index versions cohere\n",
    "\n",
    "# Cohere Playground\n",
    "# https://dashboard.cohere.com/playground/chat\n",
    "\n",
    "import cohere\n",
    "import os\n",
    "\n",
    "COHERE_API_KEY=os.environ.get(\"COHERE_API_KEY\")\n",
    "co = cohere.Client(\n",
    "  api_key=COHERE_API_KEY, # This is your trial API key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7e41820-44cc-4c13-a314-28e8d2ce1dce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed-english-light-v2.0\n",
      "embed-english-v2.0\n",
      "rerank-english-v3.0\n",
      "command-r\n",
      "embed-multilingual-light-v3.0\n",
      "command-r-plus\n",
      "embed-multilingual-v3.0\n",
      "embed-multilingual-v2.0\n",
      "command-light-nightly\n",
      "rerank-multilingual-v2.0\n",
      "embed-english-v3.0\n",
      "command\n",
      "rerank-multilingual-v3.0\n",
      "rerank-english-v2.0\n",
      "command-light\n",
      "c4ai-aya\n",
      "embed-english-light-v3.0\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import pprint\n",
    "\n",
    "# https://docs.cohere.com/reference/list-models\n",
    "models = co.models.list()\n",
    "# pprint.pprint(models)\n",
    "for model in models.models:\n",
    "  # pprint.pprint(model)\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e4ab9-e6f9-4887-b36b-a6d29696ac0e",
   "metadata": {},
   "source": [
    "### playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07556486-7fc4-41c6-8091-ed2b0b623ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aqui está uma versão revisada:\n",
      "\n",
      "Zé, quando você terminar o trabalho de escola, poderia me avisar, por favor? Estou planejando usá-lo como material de estudo para a prova e gostaria de ter acesso a ele o quanto antes. Agradeço sua gentileza em me ajudar com isso.\n",
      "\n",
      "A temperatura 1 é mantida, com uma linguagem gentil e positiva."
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\n",
    "revise e melhore o texto abaixo. Seja gentil e positivo. Use temperatura 1.\n",
    "\\\"\"\"\n",
    "Zé, assim que tiver feito o trabalho de escola me avise? Pretendo usar esse trabalho para estudar pra prova.\n",
    "\\\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = PROMPT\n",
    "stream = co.chat_stream( \n",
    "  model='command-r-plus',\n",
    "  message=prompt,\n",
    "  temperature=0.3,\n",
    "  chat_history=[],\n",
    "  prompt_truncation='AUTO',\n",
    "  connectors=[{\"id\":\"web-search\"}]\n",
    ") \n",
    "\n",
    "for event in stream:\n",
    "  if event.event_type == \"text-generation\":\n",
    "    print(event.text, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed24923-3985-4493-ad67-e8fa350791dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8469d82-5320-4726-9202-30833a2b79b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f49ed-f67e-4bbe-8af2-6cb38550e554",
   "metadata": {},
   "source": [
    "- reference:\n",
    "  - [Libraries](https://platform.openai.com/docs/libraries)\n",
    "  - [API Reference](https://platform.openai.com/docs/api-reference/introduction?lang=python)\n",
    "  - [Python library](https://platform.openai.com/docs/libraries/python-library)\n",
    "  - [OpenAI Python API library](https://github.com/openai/openai-python)\n",
    "    - _The official Python library for the OpenAI API_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae47060-ebbf-435d-b42a-445903510874",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "# pip3 install --upgrade openai==1.28.1\n",
    "# pip3 show openai\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"enter OPENAI_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "678a2adb-8a42-4f0f-98e1-70c4c14a5f76",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "clientOpenAI = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947ddca6-83bd-4f3f-a7bc-2c965d313cd4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dall-e-3\n",
      "whisper-1\n",
      "davinci-002\n",
      "dall-e-2\n",
      "gpt-3.5-turbo-16k\n",
      "tts-1-hd-1106\n",
      "tts-1-hd\n",
      "gpt-4\n",
      "gpt-4-0613\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-3.5-turbo-instruct\n",
      "tts-1\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0301\n",
      "gpt-4-turbo-2024-04-09\n",
      "babbage-002\n",
      "gpt-4-1106-preview\n",
      "gpt-4-0125-preview\n",
      "tts-1-1106\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-4-turbo-preview\n",
      "text-embedding-3-large\n",
      "text-embedding-3-small\n",
      "gpt-3.5-turbo-0613\n",
      "text-embedding-ada-002\n",
      "gpt-4-1106-vision-preview\n",
      "gpt-4-turbo\n",
      "gpt-4-vision-preview\n",
      "gpt-3.5-turbo-16k-0613\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/sigoden/aichat/blob/1e8fc5d269985048d8d3023a615b94a8908571cf/models.yaml#L79\n",
    "models = clientOpenAI.models.list()\n",
    "# print(models)\n",
    "for model in models:\n",
    "  print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfce53d-cf91-4904-b989-6c1d32e77ba3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### plaground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f72eee-2133-4ba1-a33f-fee9a60410a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the heart of code's intricate dance,\n",
      "Lies a concept that gives programmers a chance,\n",
      "Recursion, a method both clever and rare,\n",
      "A game of mirrors in the programming lair.\n",
      "\n",
      "Like a mirror reflecting its own reflection,\n",
      "Recursion calls upon its own direction,\n",
      "A function that calls itself to solve a task,\n",
      "Creating a looping, recursive mask.\n",
      "\n",
      "It's a dance of echoes, a rhythmic beat,\n",
      "Repeating steps until the code's complete,\n",
      "Like a fractal that deepens with each call,\n",
      "Recursion scales up, never to stall.\n",
      "\n",
      "From trees to lists, from stacks to queues,\n",
      "Recursion weaves through data with ease,\n",
      "A powerful tool in the coder's hand,\n",
      "Unraveling complexity, a wonderland.\n",
      "\n",
      "So embrace the loop that echoes within,\n",
      "Let recursion's magic under your skin,\n",
      "For in the world of programming's song,\n",
      "Recursion dances gracefully, forever strong.\n"
     ]
    }
   ],
   "source": [
    "completion = clientOpenAI.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "# print(completion.choices[0].message)\n",
    "# print('\\n')\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e4c0ac-98d5-452b-896f-c233fc17bdc5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da615c-a8f2-45ea-aa6b-53fc074e00c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20f505fc-529b-4720-8a72-aabb47e351d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter MISTRAL_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "# pip3 install --upgrade mistralai==0.1.8\n",
    "# pip3 show mistralai\n",
    "# https://github.com/mistralai/client-python\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "os.environ['MISTRAL_API_KEY'] = getpass.getpass(\"enter MISTRAL_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578d84ff-68e8-4cec-aab5-c0d7afa0b592",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "\n",
    "# https://docs.mistral.ai/getting-started/models/#api-versioning\n",
    "modelMistral = \"mistral-tiny\"\n",
    "\n",
    "clientMistral = MistralClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "576e6d7f-b2b1-47ff-8e17-2e618229f0c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open-mistral-7b\n",
      "mistral-tiny-2312\n",
      "mistral-tiny\n",
      "open-mixtral-8x7b\n",
      "open-mixtral-8x22b\n",
      "open-mixtral-8x22b-2404\n",
      "mistral-small-2312\n",
      "mistral-small\n",
      "mistral-small-2402\n",
      "mistral-small-latest\n",
      "mistral-medium-latest\n",
      "mistral-medium-2312\n",
      "mistral-medium\n",
      "mistral-large-latest\n",
      "mistral-large-2402\n",
      "mistral-embed\n"
     ]
    }
   ],
   "source": [
    "# https://docs.mistral.ai/getting-started/models/#api-versioning\n",
    "# https://github.com/sigoden/aichat/blob/1e8fc5d269985048d8d3023a615b94a8908571cf/models.yaml#L79\n",
    "# print(client.list_models().data)\n",
    "models = clientMistral.list_models().data\n",
    "for model in models:\n",
    "  print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9597687-f729-4754-92ae-c1114549dd5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57481b7c-dfc7-4c66-a822-f8d84cdc973e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining the \"best\" French cheese is subjective, as it depends on personal preferences, as there are various types of French cheese, each with unique flavors and textures. Here are a few popular ones, each with its distinct characteristics:\n",
      "\n",
      "1. Roquefort: A blue-veined sheep's milk cheese from the Massif Central region. It has a strong, pungent smell and a tangy, savory taste.\n",
      "\n",
      "2. Camembert: A soft, creamy, and runny cow's milk cheese from Normandy. It has a strong, earthy flavor with a distinct mushroomy aroma.\n",
      "\n",
      "3. Comté: A firm, nutty, and slightly sweet cow's milk cheese from Franche-Comté. It has a rich, complex flavor and a smooth, dense texture.\n",
      "\n",
      "4. Brie de Meaux: A soft, creamy cow's milk cheese with a velvety white rind and a mild, buttery flavor. It comes from the Île-de-France region.\n",
      "\n",
      "5. Munster: A soft, pungent, and slightly sweet cow's milk cheese from Alsace. It has a mellow, nutty flavor with a distinctive smell.\n",
      "\n",
      "To find your favorite French cheese, it's best to try a variety and taste the differences for yourself.\n"
     ]
    }
   ],
   "source": [
    "chat_response = clientMistral.chat(\n",
    "  model=modelMistral,\n",
    "  messages=[ChatMessage(role=\"user\", content=\"What is the best French cheese?\")],\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032fbb6-0040-4106-9202-32982f6929e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20212bc-7b73-417f-b10c-4770f4d4ec64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336b141d-4bbe-4e1c-8e76-9bbb7c0af727",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter ANTHROPIC_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "# pip3 install --upgrade anthropic==0.25.8\n",
    "# pip3 show anthropic\n",
    "# https://docs.anthropic.com/en/api/client-sdks#python\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "os.environ['ANTHROPIC_API_KEY'] = getpass.getpass(\"enter ANTHROPIC_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa2678e-3c3d-42dd-ad03-125c9a50abc6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# https://docs.anthropic.com/en/api/client-sdks#python\n",
    "# https://github.com/anthropics/anthropic-sdk-python\n",
    "import os\n",
    "import anthropic\n",
    "\n",
    "api_key = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "clientAnthropic = anthropic.Anthropic(\n",
    "  api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4bc0a-04b1-4880-949e-8590b9b0c3c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b34d6582-a9ab-4dd6-9328-500c35a2af69",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text=\"Hello! It's nice to meet you. How are you doing today?\", type='text')]\n"
     ]
    }
   ],
   "source": [
    "# List of Models\n",
    "# https://docs.anthropic.com/en/docs/models-overview#claude-3-a-new-generation-of-ai\n",
    "# https://github.com/sigoden/aichat/blob/1e8fc5d269985048d8d3023a615b94a8908571cf/models.yaml#L79\n",
    "message = clientAnthropic.messages.create(\n",
    "  model=\"claude-3-opus-20240229\",\n",
    "  # model=\"claude-instant-1.2\",\n",
    "  max_tokens=1024,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n",
    "  ]\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def0ace-e6e1-4a69-b182-22f846945eab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## All At Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cc05030-fed4-417b-847a-a2e357c375e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GEMINI:\n",
      "I apologize for the delay. I have been occupied with another time-consuming matter, but I am now available to assist you.\n",
      "\n",
      "\n",
      "OPEN AI:\n",
      "I apologize for the delay as I have been preoccupied with another time-consuming task. Thank you for your understanding.\n",
      "\n",
      "\n",
      "MISTRAL:\n",
      "I apologize for the delay in response. I've been focused on another project that required my full attention. Please accept my apologies for any inconvenience caused.\n",
      "\n",
      "\n",
      "CLAUDE ANTHROPIC:\n",
      "Here is my suggestion:\n",
      "\n",
      "\"Apologies for the delay in responding. I've been focused on another task recently that required more of my time and attention than anticipated. In the future, I will aim to better manage multiple priorities and communicate more promptly.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\n",
    "Proofread and improve the text below. Be positive and humble.\n",
    "\\\"\"\"\n",
    "Sorry for the delay, I have been busy with something else that was timeconsuming me.\n",
    "\\\"\"\"\n",
    "\"\"\"\n",
    "prompt = PROMPT\n",
    "\n",
    "\"\"\"\n",
    "GEMINI\n",
    "\"\"\"\n",
    "modelGemini = genai.GenerativeModel('gemini-pro')\n",
    "response = modelGemini.generate_content(prompt)\n",
    "print(f\"\"\"\n",
    "GEMINI:\n",
    "{response.text}\n",
    "\"\"\")\n",
    "\n",
    "\"\"\"\n",
    "OPEN AI\n",
    "\"\"\"\n",
    "completion = clientOpenAI.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "print(f\"\"\"\n",
    "OPEN AI:\n",
    "{completion.choices[0].message.content}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "MISTRAL\n",
    "\"\"\"\n",
    "chat_response = clientMistral.chat(\n",
    "  model=modelMistral,\n",
    "  messages=[ChatMessage(role=\"user\", content=prompt)],\n",
    ")\n",
    "print(f\"\"\"\n",
    "MISTRAL:\n",
    "{chat_response.choices[0].message.content}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "CLAUDE ANTHROPIC\n",
    "\"\"\"\n",
    "message = clientAnthropic.messages.create(\n",
    "  # model=\"claude-3-opus-20240229\",\n",
    "  model=\"claude-instant-1.2\",\n",
    "  max_tokens=1024,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "print(f\"\"\"\n",
    "CLAUDE ANTHROPIC:\n",
    "{message.content[0].text}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
