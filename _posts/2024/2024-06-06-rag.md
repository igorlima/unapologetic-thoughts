---
layout: post
title: Retrieval Augmented Generation - RAG
category: code-sample
---

RAG is a simple three-step process: indexing, retrieval, and generation.

__LlamaIndex or LangChain?__

In straightforward terms, LLamaIndex plays a crucial role in retrieving context
pieces and interacting with the LLM. It shares similarities with LangChain in
the overall process, albeit with distinctive terminology: LLamaIndex refers to
__"chunks"__ as __"nodes"__. [^2]

But, what sets LLamaIndex apart from LangChain, and how can you make an
informed choice between the two?
- In a nutshell: [^2]
  - __LLamaIndex__: offers a plethora of options for processing/chunking
    various document types and it provides a rich array of retrieval
    possibilities.
  - __LangChain__: demonstrates more flexibility, providing extensive options
    for interaction with the LLM

__Enhancing data granularity__

Metadata is useful because it brings an additional layer of structured search
on top vector search. [^1]

__Hyde or Query2doc__

Both Hyde and Query2doc are similar query rewriting optimisations. Given that
search queries are often short, ambiguous, or lack necessary background
information, LLMs can provide relevant information to guide retrieval systems,
as they memorize an enormous amount of knowledge and language patterns by
pre-training on trillions of tokens. [^1]

<details markdown="block">
<summary><i><sup>difference between Standard and Hyde approach</sup></i></summary>

</details>

---
{: data-content="footnotes"}

[^1]: [Advance RAG- Improve RAG performance](https://luv-bansal.medium.com/advance-rag-improve-rag-performance-208ffad5bb6a)
[^2]: [Offline RAG with LlamaIndex and tiny/small LLMs](https://medium.com/pythoneers/offline-rag-with-llamaindex-and-tiny-and-small-llms-ab2acac936b0)
