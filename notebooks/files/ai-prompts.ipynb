{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4581b95f-ee6f-46ee-ba6d-75e61f6db310",
   "metadata": {},
   "source": [
    "# AI Prompts\n",
    "\n",
    "- re-render Markdown preview after an edition:\n",
    "  - __\"Activate Command Pallete\"__ by pressing `Ctrl+Shift+C`\n",
    "    - _it's also located under the menu \"View\"_\n",
    "  - type __\"Render All Markdown Cells\"__\n",
    "  - press `Enter`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc8987-e773-42a2-8554-4ff6bec0e258",
   "metadata": {},
   "source": [
    "## Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c15ee4-5757-4fe5-a84e-aaff8247795b",
   "metadata": {},
   "source": [
    "\n",
    "- How to Create a Virtual Environment and Use it on Jupyter Notebook\n",
    "  - https://towardsdatascience.com/how-to-create-a-virtual-environment-and-use-it-on-jupyter-notebook-6c0b7b1cfca0\n",
    "  - https://medium.com/@kishanck/virtual-environments-for-jupyter-notebooks-847b7a3b4da0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae0fbe7-7848-40fb-a950-4ee1f2c715ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### using passwords in Jupyter notebooks\n",
    "\n",
    "In Python you can ask the user for input via the `input` function.\n",
    "```python\n",
    "pwd = input(\"Password:\")\n",
    "```\n",
    "\n",
    "When you run this command locally, here's what it might look like:\n",
    "\n",
    "```\n",
    ">>> pwd = input(\"Password:\")\n",
    "Password: supersecret\n",
    "```\n",
    "\n",
    "The `pwd` variable will contain the string \"supersecret\", but notice how the command prompt actually shows what the user is typing! That means that somebody who is sitting next to you, or looking at your screen over zoom, also can read your password! That's bad.\n",
    "\n",
    "#### getpass\n",
    "\n",
    "For situations like this one, you may enjoy using the getpass module in Python instead. It has the same functionality but won't display the typed password.\n",
    "\n",
    "```\n",
    ">>> import getpass\n",
    ">>> pwd = getpass.getpass(\"give password\")\n",
    "Password:üóùÔ∏è\n",
    "```\n",
    "\n",
    "No matter what you type, it won't be printed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf2730-0609-46df-bd59-375c58317329",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3df19-c1b6-48d8-be0f-c10cc5e9dc5e",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2450908-88c6-444c-93b4-7e6ba6d3604b",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eccae4-09ea-42db-b8a1-ed3d7ac2253e",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f97c55-251f-4893-8342-062bf7bf9e65",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cabad1f-e4f7-45e1-8d93-a6c3ef886d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter GOOGLE_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ['GOOGLE_API_KEY'] = getpass.getpass(\"enter GOOGLE_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3980015e-b849-4bd0-8115-3bab1af7df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install -I google-generativeai==0.5.2\n",
    "# pip3 install --upgrade --force-reinstall google-generativeai\n",
    "# pip3 show google-generativeai\n",
    "# pip3 index versions google-generativeai\n",
    "\n",
    "# Overview of Generative AI on Vertex AI:\n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/learn/overview\n",
    "# Google Gemini Documentation:\n",
    "# https://github.com/google/generative-ai-docs?tab=readme-ov-file\n",
    "\n",
    "import google.generativeai as genai\n",
    "import google.ai.generativelanguage as glm\n",
    "import os\n",
    "\n",
    "GOOGLE_API_KEY=os.environ.get(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c55a5f01-594a-4088-bf04-1e4af9113987",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# https://github.com/sigoden/aichat/blob/1e8fc5d269985048d8d3023a615b94a8908571cf/models.yaml#L79\n",
    "for model in genai.list_models():\n",
    "  # pprint.pprint(model)\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c27937-9d61-4df5-b765-e574995d52f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c1b89-7784-4a1d-9315-8ca1e72ee5e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile prompt.txt\n",
    "What's your name? Is it Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae0207-f63b-4858-9745-e9cb2e17945f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "PROMPT_FROM_FILE = \"\"\n",
    "with open(\"prompt.txt\", \"r\") as file:\n",
    "  content = file.read()\n",
    "  PROMPT_FROM_FILE=content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a98f817d-0061-43f3-90d6-dda4c78bdd4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, my name is Gemini, a multi-modal AI language model developed by Google. My name is not Gemini.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\n",
    "What's your name? Is it Gemini\n",
    "\\\"\"\"\n",
    "\"\"\"\n",
    "CONTEXT = None\n",
    "\n",
    "prompt = PROMPT\n",
    "# prompt = PROMPT_FROM_FILE\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "# response = model.generate_content(prompt)\n",
    "response = model.generate_content(glm.Content(\n",
    "  parts = [\n",
    "    glm.Part(text=prompt),\n",
    "    glm.Part(text=CONTEXT if CONTEXT is not None else ''),\n",
    "  ],\n",
    "))\n",
    "print(response.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f029c-a3ec-4ba5-a3da-1065aba89039",
   "metadata": {},
   "source": [
    "### Cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422abc53-123d-4577-93f5-9f7db26098ca",
   "metadata": {},
   "source": [
    "Calls made using Trial keys are free of charge. Trial keys are rate-limited, and cannot be used for commercial purposes.\n",
    "- https://dashboard.cohere.com/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd23728-2be9-4ced-b30c-2771b762a22a",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7686abe2-7430-461d-9f25-6069b4d51fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter COHERE_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ['COHERE_API_KEY'] = getpass.getpass(\"enter COHERE_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d98f95-f330-4a6d-936f-8c41554f10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install -I cohere==5.5.0\n",
    "# pip3 install --upgrade --force-reinstall cohere\n",
    "# pip3 show cohere\n",
    "# pip3 index versions cohere\n",
    "\n",
    "# Cohere Playground\n",
    "# https://dashboard.cohere.com/playground/chat\n",
    "\n",
    "import cohere\n",
    "import os\n",
    "\n",
    "COHERE_API_KEY=os.environ.get(\"COHERE_API_KEY\")\n",
    "co = cohere.Client(\n",
    "  api_key=COHERE_API_KEY, # This is your trial API key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7e41820-44cc-4c13-a314-28e8d2ce1dce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed-english-light-v2.0\n",
      "embed-english-v2.0\n",
      "rerank-english-v3.0\n",
      "command-r\n",
      "embed-multilingual-light-v3.0\n",
      "command-r-plus\n",
      "embed-multilingual-v3.0\n",
      "embed-multilingual-v2.0\n",
      "command-light-nightly\n",
      "rerank-multilingual-v2.0\n",
      "embed-english-v3.0\n",
      "command\n",
      "rerank-multilingual-v3.0\n",
      "rerank-english-v2.0\n",
      "command-light\n",
      "c4ai-aya\n",
      "embed-english-light-v3.0\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import pprint\n",
    "\n",
    "# https://docs.cohere.com/reference/list-models\n",
    "models = co.models.list()\n",
    "# pprint.pprint(models)\n",
    "for model in models.models:\n",
    "  # pprint.pprint(model)\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e4ab9-e6f9-4887-b36b-a6d29696ac0e",
   "metadata": {},
   "source": [
    "#### playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0026f160-ce9a-4e82-85fe-1af1e9aab8a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "PROMPT_FROM_FILE = \"\"\n",
    "with open(\"prompt.txt\", \"r\") as file:\n",
    "  content = file.read()\n",
    "  PROMPT_FROM_FILE=content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07556486-7fc4-41c6-8091-ed2b0b623ab9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Coral. I am an AI-assistant chatbot developed to help users by providing thorough responses. I am powered by Command, a large language model built by the company Cohere. It's nice to meet you! Is there anything I can help you with today?"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\n",
    "revise e melhore o texto abaixo. Seja gentil e positivo. Use temperatura 1.\n",
    "\\\"\"\"\n",
    "Z√©, assim que tiver feito o trabalho de escola me avise? Pretendo usar esse trabalho para estudar pra prova.\n",
    "\\\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# prompt = PROMPT\n",
    "prompt = PROMPT_FROM_FILE\n",
    "stream = co.chat_stream( \n",
    "  model='command-r-plus',\n",
    "  message=prompt,\n",
    "  temperature=0.3,\n",
    "  chat_history=[],\n",
    "  prompt_truncation='AUTO',\n",
    "  connectors=[{\"id\":\"web-search\"}]\n",
    ") \n",
    "\n",
    "for event in stream:\n",
    "  if event.event_type == \"text-generation\":\n",
    "    print(event.text, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313818aa-2c54-4577-b9cb-f3bdc382ad57",
   "metadata": {},
   "source": [
    "### groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaaa2b0-b5ed-4713-88c3-83e44e6c6db9",
   "metadata": {},
   "source": [
    "- https://groq.com/\n",
    "- https://console.groq.com/playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2315ed96-4a02-43d6-8f66-c63398a1be91",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c14da8e6-6721-4235-a112-c4a4bc12b979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter GROQ_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ['GROQ_API_KEY'] = getpass.getpass(\"enter GROQ_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3964535e-5462-4038-8815-75b78a1a7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install -I groq==0.8.0\n",
    "# pip3 install --upgrade --force-reinstall groq\n",
    "# pip3 show groq\n",
    "# pip3 index versions groq\n",
    "\n",
    "# GROQ Playground\n",
    "# https://console.groq.com/playground\n",
    "# https://groq.com/\n",
    "\n",
    "import os\n",
    "from groq import Groq\n",
    "groq_client = Groq(\n",
    "  # This is the default and can be omitted\n",
    "  api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "896491d3-42ed-4df0-944b-f5ca6468bc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'active': True,\n",
      "           'context_window': 8192,\n",
      "           'created': 1693721698,\n",
      "           'id': 'gemma-7b-it',\n",
      "           'object': 'model',\n",
      "           'owned_by': 'Google'},\n",
      "          {'active': True,\n",
      "           'context_window': 8192,\n",
      "           'created': 1693721698,\n",
      "           'id': 'llama3-70b-8192',\n",
      "           'object': 'model',\n",
      "           'owned_by': 'Meta'},\n",
      "          {'active': True,\n",
      "           'context_window': 8192,\n",
      "           'created': 1693721698,\n",
      "           'id': 'llama3-8b-8192',\n",
      "           'object': 'model',\n",
      "           'owned_by': 'Meta'},\n",
      "          {'active': True,\n",
      "           'context_window': 32768,\n",
      "           'created': 1693721698,\n",
      "           'id': 'mixtral-8x7b-32768',\n",
      "           'object': 'model',\n",
      "           'owned_by': 'Mistral AI'}],\n",
      " 'object': 'list'}\n"
     ]
    }
   ],
   "source": [
    "# MODELS\n",
    "# https://console.groq.com/docs/models\n",
    "\"\"\"\n",
    "curl -X GET \"https://api.groq.com/openai/v1/models\" \\\n",
    "     -H \"Authorization: Bearer $GROQ_API_KEY\" \\\n",
    "     -H \"Content-Type: application/json\"\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "headers = {\n",
    "  \"Authorization\": f\"Bearer {api_key}\",\n",
    "  \"Content-Type\": \"application/json\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264a8ca-2260-45d5-acd7-c822306d4dd3",
   "metadata": {},
   "source": [
    "#### playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4d16aec-ccb9-4daf-9ffc-b92cae113385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's revise the text together\n",
      "\n",
      "Here's a revised version with a gentle and positive tone:\n",
      "\n",
      "\"Hey, don't forget to remind me when the schoolwork is done so we can study for the exam together!\"\n",
      "\n",
      "How does that sound?\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\n",
    "revise e melhore o texto abaixo. Seja gentil e positivo. Use temperatura 1.\n",
    "\\\"\"\"\n",
    "Z√©, assim que tiver feito o trabalho de escola me avise? Pretendo usar esse trabalho para estudar pra prova.\n",
    "\\\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# prompt = PROMPT\n",
    "prompt = PROMPT\n",
    "chat_completion = groq_client.chat.completions.create(\n",
    "  messages=[{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"you are a helpful assistant.\"\n",
    "  }, {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": prompt,\n",
    "  }],\n",
    "  model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38a739-5a58-4803-81e5-7e7cca7df6c8",
   "metadata": {},
   "source": [
    "### perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d276b-22fd-4f3d-bb38-a58260040697",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5178b42d-e004-4776-8c2e-68e39c87e9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter PERPLEXITY_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ['PERPLEXITY_API_KEY'] = getpass.getpass(\"enter PERPLEXITY_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c735701-ceeb-4fdb-a6f3-5d51a1db606d",
   "metadata": {},
   "source": [
    "#### playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bb1bde3-1a2f-4d8a-ac9a-4342231572a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of stars in the universe is a mind-boggling topic that has fascinated humans for centuries. According to estimates from various sources, including NASA and the European Space Agency (ESA), the universe could contain up to **one septillion stars** ‚Äì that's a one followed by 24 zeros. This number is based on the assumption that there are approximately **2 trillion galaxies** in the observable universe, with each galaxy containing around **100 billion stars** on average.\n",
      "\n",
      "To put this number into perspective, if we were to count the stars in the universe at a rate of one star per second, it would take us over **6,000 years** to count just the stars in the Milky Way galaxy alone, which is estimated to have over **100 billion stars**. The sheer scale of the universe and the number of stars it contains is truly awe-inspiring.\n",
      "\n",
      "It's worth noting that these estimates are rough and based on current observations and understanding of the universe. As new data and missions become available, our understanding of the universe and its contents may change, potentially leading to revised estimates of the number of stars.\n",
      "\n",
      "In summary, the number of stars in the universe is estimated to be around **200 billion trillion**, or **200 sextillion**, which is an almost incomprehensible number that highlights the vastness and complexity of the cosmos.\n",
      "---\n",
      "The number of stars in the universe is a staggering and difficult to comprehend figure. According to various estimates, there are approximately **200 billion trillion stars** in the universe. This number is derived by multiplying the estimated number of galaxies in the universe (around 2 trillion) by the average number of stars in a typical galaxy (about 100 billion).\n",
      "\n",
      "The Milky Way, which is just one of the galaxies in the universe, is estimated to contain around **100 billion stars**. This number is based on observations of the galaxy's structure and the diversity of its stars, which come in different sizes and colors. Our Sun is a medium-sized, medium-weight, and medium-hot star, with a surface temperature of about 27 million degrees Fahrenheit (15 million degrees Celsius).\n",
      "\n",
      "The process of counting the stars in the universe involves several steps. First, astronomers estimate the number of galaxies in the universe by taking detailed pictures of small parts of the sky and counting the galaxies in those pictures. They then multiply this number by the number of pictures needed to photograph the whole sky. Next, they estimate the number of stars in a typical galaxy, like the Milky Way, by measuring the starlight and its color and brightness. Finally, they multiply the number of stars in a typical galaxy by the number of galaxies in the universe to get the total number of stars.\n",
      "\n",
      "It's worth noting that these estimates are rough and based on current observations and understanding of the universe. The actual number of stars could be higher or lower, and new discoveries and missions, such as the Gaia mission, are helping to refine our understanding of the universe and its contents."
     ]
    }
   ],
   "source": [
    "# https://docs.perplexity.ai/docs/getting-started\n",
    "# https://docs.perplexity.ai/docs/model-cards\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "messages = [{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": (\n",
    "    \"You are an artificial intelligence assistant and you need to \"\n",
    "    \"engage in a helpful, detailed, polite conversation with a user.\"\n",
    "  ),\n",
    "}, {\n",
    "  \"role\": \"user\",\n",
    "  \"content\": (\n",
    "    \"How many stars are in the universe?\"\n",
    "  ),\n",
    "}]\n",
    "\n",
    "perplexity_client = OpenAI(api_key=os.environ.get(\"PERPLEXITY_API_KEY\"), base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "# chat completion without streaming\n",
    "response = perplexity_client.chat.completions.create(\n",
    "  model=\"llama-3-sonar-large-32k-online\",\n",
    "  messages=messages,\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print('---')\n",
    "# chat completion with streaming\n",
    "response_stream = perplexity_client.chat.completions.create(\n",
    "  model=\"llama-3-sonar-large-32k-online\",\n",
    "  messages=messages,\n",
    "  stream=True,\n",
    ")\n",
    "for response in response_stream:\n",
    "  print(response.choices[0].delta.content, end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed24923-3985-4493-ad67-e8fa350791dc",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8469d82-5320-4726-9202-30833a2b79b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f49ed-f67e-4bbe-8af2-6cb38550e554",
   "metadata": {},
   "source": [
    "- reference:\n",
    "  - [Libraries](https://platform.openai.com/docs/libraries)\n",
    "  - [API Reference](https://platform.openai.com/docs/api-reference/introduction?lang=python)\n",
    "  - [Python library](https://platform.openai.com/docs/libraries/python-library)\n",
    "  - [OpenAI Python API library](https://github.com/openai/openai-python)\n",
    "    - _The official Python library for the OpenAI API_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae47060-ebbf-435d-b42a-445903510874",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter OPENAI_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "# pip3 install --upgrade openai==1.28.1\n",
    "# pip3 show openai\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"enter OPENAI_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "678a2adb-8a42-4f0f-98e1-70c4c14a5f76",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "clientOpenAI = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947ddca6-83bd-4f3f-a7bc-2c965d313cd4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dall-e-3\n",
      "whisper-1\n",
      "davinci-002\n",
      "dall-e-2\n",
      "gpt-3.5-turbo-16k\n",
      "tts-1-hd-1106\n",
      "tts-1-hd\n",
      "gpt-4\n",
      "gpt-4-0613\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-3.5-turbo-instruct\n",
      "tts-1\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0301\n",
      "gpt-4-turbo-2024-04-09\n",
      "babbage-002\n",
      "gpt-4-1106-preview\n",
      "gpt-4-0125-preview\n",
      "tts-1-1106\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-4-turbo-preview\n",
      "text-embedding-3-large\n",
      "text-embedding-3-small\n",
      "gpt-3.5-turbo-0613\n",
      "text-embedding-ada-002\n",
      "gpt-4-1106-vision-preview\n",
      "gpt-4-turbo\n",
      "gpt-4-vision-preview\n",
      "gpt-3.5-turbo-16k-0613\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/sigoden/aichat/blob/1e8fc5d269985048d8d3023a615b94a8908571cf/models.yaml#L79\n",
    "models = clientOpenAI.models.list()\n",
    "# print(models)\n",
    "for model in models:\n",
    "  print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfce53d-cf91-4904-b989-6c1d32e77ba3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### plaground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f72eee-2133-4ba1-a33f-fee9a60410a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the heart of code's intricate dance,\n",
      "Lies a concept that gives programmers a chance,\n",
      "Recursion, a method both clever and rare,\n",
      "A game of mirrors in the programming lair.\n",
      "\n",
      "Like a mirror reflecting its own reflection,\n",
      "Recursion calls upon its own direction,\n",
      "A function that calls itself to solve a task,\n",
      "Creating a looping, recursive mask.\n",
      "\n",
      "It's a dance of echoes, a rhythmic beat,\n",
      "Repeating steps until the code's complete,\n",
      "Like a fractal that deepens with each call,\n",
      "Recursion scales up, never to stall.\n",
      "\n",
      "From trees to lists, from stacks to queues,\n",
      "Recursion weaves through data with ease,\n",
      "A powerful tool in the coder's hand,\n",
      "Unraveling complexity, a wonderland.\n",
      "\n",
      "So embrace the loop that echoes within,\n",
      "Let recursion's magic under your skin,\n",
      "For in the world of programming's song,\n",
      "Recursion dances gracefully, forever strong.\n"
     ]
    }
   ],
   "source": [
    "completion = clientOpenAI.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "# print(completion.choices[0].message)\n",
    "# print('\\n')\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e4c0ac-98d5-452b-896f-c233fc17bdc5",
   "metadata": {},
   "source": [
    "### Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da615c-a8f2-45ea-aa6b-53fc074e00c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20f505fc-529b-4720-8a72-aabb47e351d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter MISTRAL_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "# pip3 install --upgrade mistralai==0.1.8\n",
    "# pip3 show mistralai\n",
    "# https://github.com/mistralai/client-python\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "os.environ['MISTRAL_API_KEY'] = getpass.getpass(\"enter MISTRAL_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578d84ff-68e8-4cec-aab5-c0d7afa0b592",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "\n",
    "# https://docs.mistral.ai/getting-started/models/#api-versioning\n",
    "modelMistral = \"mistral-tiny\"\n",
    "\n",
    "clientMistral = MistralClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "576e6d7f-b2b1-47ff-8e17-2e618229f0c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open-mistral-7b\n",
      "mistral-tiny-2312\n",
      "mistral-tiny\n",
      "open-mixtral-8x7b\n",
      "open-mixtral-8x22b\n",
      "open-mixtral-8x22b-2404\n",
      "mistral-small-2312\n",
      "mistral-small\n",
      "mistral-small-2402\n",
      "mistral-small-latest\n",
      "mistral-medium-latest\n",
      "mistral-medium-2312\n",
      "mistral-medium\n",
      "mistral-large-latest\n",
      "mistral-large-2402\n",
      "mistral-embed\n"
     ]
    }
   ],
   "source": [
    "# https://docs.mistral.ai/getting-started/models/#api-versioning\n",
    "# https://github.com/sigoden/aichat/blob/1e8fc5d269985048d8d3023a615b94a8908571cf/models.yaml#L79\n",
    "# print(client.list_models().data)\n",
    "models = clientMistral.list_models().data\n",
    "for model in models:\n",
    "  print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9597687-f729-4754-92ae-c1114549dd5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57481b7c-dfc7-4c66-a822-f8d84cdc973e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining the \"best\" French cheese is subjective, as it depends on personal preferences, as there are various types of French cheese, each with unique flavors and textures. Here are a few popular ones, each with its distinct characteristics:\n",
      "\n",
      "1. Roquefort: A blue-veined sheep's milk cheese from the Massif Central region. It has a strong, pungent smell and a tangy, savory taste.\n",
      "\n",
      "2. Camembert: A soft, creamy, and runny cow's milk cheese from Normandy. It has a strong, earthy flavor with a distinct mushroomy aroma.\n",
      "\n",
      "3. Comt√©: A firm, nutty, and slightly sweet cow's milk cheese from Franche-Comt√©. It has a rich, complex flavor and a smooth, dense texture.\n",
      "\n",
      "4. Brie de Meaux: A soft, creamy cow's milk cheese with a velvety white rind and a mild, buttery flavor. It comes from the √éle-de-France region.\n",
      "\n",
      "5. Munster: A soft, pungent, and slightly sweet cow's milk cheese from Alsace. It has a mellow, nutty flavor with a distinctive smell.\n",
      "\n",
      "To find your favorite French cheese, it's best to try a variety and taste the differences for yourself.\n"
     ]
    }
   ],
   "source": [
    "chat_response = clientMistral.chat(\n",
    "  model=modelMistral,\n",
    "  messages=[ChatMessage(role=\"user\", content=\"What is the best French cheese?\")],\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032fbb6-0040-4106-9202-32982f6929e1",
   "metadata": {},
   "source": [
    "### Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20212bc-7b73-417f-b10c-4770f4d4ec64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336b141d-4bbe-4e1c-8e76-9bbb7c0af727",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter ANTHROPIC_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "# pip3 install --upgrade anthropic==0.25.8\n",
    "# pip3 show anthropic\n",
    "# https://docs.anthropic.com/en/api/client-sdks#python\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "os.environ['ANTHROPIC_API_KEY'] = getpass.getpass(\"enter ANTHROPIC_API_KEY: \")\n",
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa2678e-3c3d-42dd-ad03-125c9a50abc6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# https://docs.anthropic.com/en/api/client-sdks#python\n",
    "# https://github.com/anthropics/anthropic-sdk-python\n",
    "import os\n",
    "import anthropic\n",
    "\n",
    "api_key = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "clientAnthropic = anthropic.Anthropic(\n",
    "  api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4bc0a-04b1-4880-949e-8590b9b0c3c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b34d6582-a9ab-4dd6-9328-500c35a2af69",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text=\"Hello! It's nice to meet you. How are you doing today?\", type='text')]\n"
     ]
    }
   ],
   "source": [
    "# List of Models\n",
    "# https://docs.anthropic.com/en/docs/models-overview#claude-3-a-new-generation-of-ai\n",
    "# https://github.com/sigoden/aichat/blob/1e8fc5d269985048d8d3023a615b94a8908571cf/models.yaml#L79\n",
    "message = clientAnthropic.messages.create(\n",
    "  model=\"claude-3-opus-20240229\",\n",
    "  # model=\"claude-instant-1.2\",\n",
    "  max_tokens=1024,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n",
    "  ]\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def0ace-e6e1-4a69-b182-22f846945eab",
   "metadata": {},
   "source": [
    "### All At Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5acb5c-cfe1-44b7-a62d-b4465d2cd650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GEMINI\n",
    "\"\"\"\n",
    "def llm_gemini(prompt):\n",
    "  modelGemini = genai.GenerativeModel('gemini-pro')\n",
    "  response = modelGemini.generate_content(prompt)\n",
    "  print(f\"\"\"\n",
    "  GEMINI:\n",
    "  {response.text}\n",
    "  \"\"\")\n",
    "\n",
    "\"\"\"\n",
    "COHERE\n",
    "\"\"\"\n",
    "def llm_cohere(prompt):\n",
    "  stream = co.chat_stream(\n",
    "    model='command-r-plus',\n",
    "    message=prompt,\n",
    "    temperature=0.3,\n",
    "    chat_history=[],\n",
    "    prompt_truncation='AUTO',\n",
    "    connectors=[{\"id\":\"web-search\"}]\n",
    "  )\n",
    "  print(f\"\"\"\n",
    "  COHERE:\n",
    "  \"\"\")\n",
    "  for event in stream:\n",
    "    if event.event_type == \"text-generation\":\n",
    "      print(event.text, end='')\n",
    "  print(\"\")\n",
    "\n",
    "\"\"\"\n",
    "GROQ\n",
    "\"\"\"\n",
    "def llm_groq(prompt):\n",
    "  chat_completion = groq_client.chat.completions.create(\n",
    "    messages=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt,\n",
    "    }],\n",
    "    model=\"llama3-8b-8192\",\n",
    "  )\n",
    "  print(f\"\"\"\n",
    "  GROQ:\n",
    "  {chat_completion.choices[0].message.content}\n",
    "  \"\"\")\n",
    "\n",
    "\"\"\"\n",
    "PERPLEXITY\n",
    "\"\"\"\n",
    "def llm_perplexity(prompt):\n",
    "  response = perplexity_client.chat.completions.create(\n",
    "    model=\"llama-3-sonar-large-32k-online\",\n",
    "    messages=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": (\n",
    "        prompt\n",
    "      ),\n",
    "    }],\n",
    "  )\n",
    "  print(f\"\"\"\n",
    "  PERPLEXITY:\n",
    "  {response.choices[0].message.content}\n",
    "  \"\"\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OPEN AI\n",
    "\"\"\"\n",
    "def llm_openai(prompt):\n",
    "  completion = clientOpenAI.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "  )\n",
    "  print(f\"\"\"\n",
    "  OPEN AI:\n",
    "  {completion.choices[0].message.content}\n",
    "  \"\"\"\n",
    "  )\n",
    "\n",
    "\"\"\"\n",
    "MISTRAL\n",
    "\"\"\"\n",
    "def llm_mistral(prompt):\n",
    "  chat_response = clientMistral.chat(\n",
    "    model=modelMistral,\n",
    "    messages=[ChatMessage(role=\"user\", content=prompt)],\n",
    "  )\n",
    "  print(f\"\"\"\n",
    "  MISTRAL:\n",
    "  {chat_response.choices[0].message.content}\n",
    "  \"\"\"\n",
    "  )\n",
    "\n",
    "\"\"\"\n",
    "CLAUDE ANTHROPIC\n",
    "\"\"\"\n",
    "def llm_claude(prompt):\n",
    "  message = clientAnthropic.messages.create(\n",
    "    # model=\"claude-3-opus-20240229\",\n",
    "    model=\"claude-instant-1.2\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "  )\n",
    "  print(f\"\"\"\n",
    "  CLAUDE ANTHROPIC:\n",
    "  {message.content[0].text}\n",
    "  \"\"\")\n",
    "\n",
    "LLM = {\n",
    "  \"GEMINI\": llm_gemini,\n",
    "  \"COHERE\": llm_cohere,\n",
    "  \"GROQ\": llm_groq,\n",
    "  \"PERPLEXITY\": llm_perplexity,\n",
    "  \"OPENAI\": llm_openai,\n",
    "  \"MISTRAL\": llm_mistral,\n",
    "  \"CLAUDE\": llm_claude\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cc05030-fed4-417b-847a-a2e357c375e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  GEMINI:\n",
      "  Sure, here is an improved version of the text:\n",
      "\n",
      "\"Thank you for your patience. I apologize for the delay in my response. I have been preoccupied with another matter that has required a significant amount of my time and attention.\"\n",
      "  \n",
      "\n",
      "  COHERE:\n",
      "  \n",
      "Sure! Here is a revised version: \n",
      "\n",
      "\"My apologies for the delay. I was caught up with some time-consuming tasks, but I am back on track now and ready to assist you.\" \n",
      "\n",
      "This version maintains a positive and humble tone while also conveying a sense of professionalism and respect for the recipient's time. It also sets a polite and friendly tone for further communication.\n",
      "\n",
      "  GROQ:\n",
      "  What a great start! It's truly wonderful to see you're reaching out. Here's a refined version that captures your tone and intention while making it more polished:\n",
      "\n",
      "\"I wanted to apologize for the delay. I've been tackling a personal project that consumed most of my time, and I didn't want to compromise on quality. Your patience and understanding are greatly appreciated!\"\n",
      "\n",
      "Feel free to modify it to fit your personal style and tone. Remember, I'm here to help!\n",
      "  \n",
      "\n",
      "  PERPLEXITY:\n",
      "  Here's a revised version of the text:\n",
      "\n",
      "\"I apologize for the delay. I've been fully engaged in another important task that required my attention. I'm now refocused and ready to move forward.\"\n",
      "\n",
      "This revised text still acknowledges the delay and expresses regret, but it does so in a more professional and polite manner. It also provides a brief explanation for the delay without making excuses, and it ends on a positive note by indicating that you're now ready to move forward.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\n",
    "Proofread and improve the text below. Be positive and humble.\n",
    "\\\"\"\"\n",
    "Sorry for the delay, I have been busy with something else that was timeconsuming me.\n",
    "\\\"\"\"\n",
    "\"\"\"\n",
    "prompt = PROMPT\n",
    "\n",
    "for llm in [\"GEMINI\", \"COHERE\", \"GROQ\", \"PERPLEXITY\"]:\n",
    "# for llm in [\"PERPLEXITY\"]:\n",
    "  LLM[llm](prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
