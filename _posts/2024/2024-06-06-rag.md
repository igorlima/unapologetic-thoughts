---
layout: post
title: Retrieval Augmented Generation - RAG
category: code-sample
---

RAG is a simple three-step process: indexing, retrieval, and generation.

__LlamaIndex or LangChain?__

In straightforward terms, LLamaIndex plays a crucial role in retrieving context
pieces and interacting with the LLM. It shares similarities with LangChain in
the overall process, albeit with distinctive terminology: LLamaIndex refers to
__"chunks"__ as __"nodes"__. [^2]

But, what sets LLamaIndex apart from LangChain, and how can you make an
informed choice between the two?
- In a nutshell: [^2]
  - __LLamaIndex__: offers a plethora of options for processing/chunking
    various document types and it provides a rich array of retrieval
    possibilities.
  - __LangChain__: demonstrates more flexibility, providing extensive options
    for interaction with the LLM

__DSPy vs LangChain & LlamaIndex__ [^4]

LangChain and LlamaIndex specialise in high-level application development,
offering ready-to-use application modules that seamlessly integrate with your
data or configuration.

If you prefer using a standard prompt for tasks like question answering over
PDFs or converting text to SQL, these libraries provide a comprehensive
ecosystem to explore.

In essence, Declarative Self-Improving Language Programs (DSPy) is designed for scenarios where you require a _lightweight,
self-optimising programming_ model rather than relying on pre-defined prompts
and integrations.

__Enhancing data granularity__

Metadata is useful because it brings an additional layer of structured search
on top vector search. [^1]

__Hyde or Query2doc__

Both Hyde and Query2doc are similar query rewriting optimisations. Given that
search queries are often short, ambiguous, or lack necessary background
information, LLMs can provide relevant information to guide retrieval systems,
as they memorize an enormous amount of knowledge and language patterns by
pre-training on trillions of tokens. [^1]

The essence of HyDE (Hypothetical Document Embeddings) is to use LLM to
generate hypothetical documents for user queries. These documents are generated
based on the knowledge of LLM itself and may contain errors or inaccuracies.
However, they are associated with the documents in the knowledge base of RAG.
Then, by using these hypothetical documents to retrieve real documents with
similar vectors, the accuracy of retrieval is improved. [^3]

<details markdown="block">
<summary><i><sup>difference between Standard and Hyde approach</sup></i></summary>

![image](https://github.com/igorlima/unapologetic-thoughts/assets/1886786/cb621582-751b-4a77-bf6b-05da6b7a86b5)

</details>

---
{: data-content="footnotes"}

[^1]: [Advance RAG- Improve RAG performance](https://luv-bansal.medium.com/advance-rag-improve-rag-performance-208ffad5bb6a)
[^2]: [Offline RAG with LlamaIndex and tiny/small LLMs](https://medium.com/pythoneers/offline-rag-with-llamaindex-and-tiny-and-small-llms-ab2acac936b0)
[^3]: [Advanced RAG Retrieval Strategy: Query Rewriting](https://generativeai.pub/advanced-rag-retrieval-strategy-query-rewriting-a1dd61815ff0)
[^4]: [An Introduction To DSPy](https://cobusgreyling.medium.com/an-introduction-to-dspy-00306973acbc)
